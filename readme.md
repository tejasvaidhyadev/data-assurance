# data-assurance

Tool kit to analysis the quality of data. (currenlty only contains scritps)

The toolkits will contain the following:

- Data validation: This involves checking data for accuracy, completeness, and consistency, and ensuring that it meets certain standards and guidelines.

- Data cleansing: This involves removing or correcting inaccurate, incomplete, or duplicate data.

- Data standardization: This involves ensuring that data is formatted and structured consistently, using common conventions and standards.

- Data enrichment: This involves adding additional information or context to data to make it more meaningful and useful.

- Data profiling: This involves analyzing data to identify patterns, trends, and issues that may impact the quality of the data.

- Data governance: This involves establishing policies, procedures, and standards for managing and maintaining data quality.

- Data documentation : This involves creating and maintaining documentation for the data, including details about its origin, format, and quality.

- Auditing and testing : This involves testing the data for quality and accuracy and auditing the data processes to identify and correct any issues that arise.

## Planned development

- I will building this tool kit in my free time, so it will take some time to complete. Feel free to contribute and always welcome to suggestions or collaborations.

More information coming soon.


## Instructions for using the paraphrasing dataset

python data_cleaning.py path_to_most_recent_cleaned_data 

#to avoid repearting the same data

## Terminal usage example:

 Original:  What is the best book about digital marketing?
 Paraphrase:  Which are best books on digital marketing?
 Is paraphrase? (y/n):  `` y``
 
 Valid keys: 
 y = Yes, n = No, q = Quit
